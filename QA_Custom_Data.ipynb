{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNykc30ejbSdjRqCq9Svk0m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hassansardar193/Natural_Language_Processing-Question_Answering/blob/main/QA_Custom_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "4XwF3QaI5WWN",
        "outputId": "379cc8d7-abe8-4180-dd3c-01ed06d4b713"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f577cd3b9f8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Define the input and output sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0minput_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0moutput_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0moutput_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'max_len' is not defined"
          ]
        }
      ],
      "source": [
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Input\n",
        "from keras.models import Model\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "# Define the input and output sequences\n",
        "input_sequence = Input((max_len,))\n",
        "output_sequence = Embedding(vocab_size, 100, input_length=max_len)(input_sequence)\n",
        "output_sequence = LSTM(128)(output_sequence)\n",
        "output_sequence = Dropout(0.5)(output_sequence)\n",
        "output_sequence = Dense(64, activation='relu')(output_sequence)\n",
        "output_sequence = Dense(1, activation='sigmoid')(output_sequence)\n",
        "\n",
        "# Create the model\n",
        "model = Model(input_sequence, output_sequence)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=10)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import allennlp\n",
        "from allennlp.predictors import Predictor\n",
        "from allennlp.data import DatasetReader\n",
        "from allennlp.models import Model\n",
        "from allennlp.training import Trainer\n",
        "from allennlp.common.util import JsonDict\n",
        "\n",
        "# Define a custom dataset reader to read in your dataset\n",
        "class CustomDatasetReader(DatasetReader):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    def _read(self, file_path: str) -> Iterable[JsonDict]:\n",
        "        # Read in and process your dataset here\n",
        "        # ...\n",
        "        # Return an iterable of dictionaries, where each dictionary contains the fields you need for your model\n",
        "        # ...\n",
        "        \n",
        "# Define your custom model\n",
        "class CustomModel(Model):\n",
        "  def __init__(self):\n",
        "        super().__init__()\n",
        "  def forward(self, *inputs) -> Dict[str, torch.Tensor]:\n",
        "        # Define the forward pass of your model here\n",
        "        # ...\n",
        "        \n",
        "# Create an instance of your custom dataset reader\n",
        "dataset_reader = CustomDatasetReader()\n",
        "# Read in your training dataset\n",
        "train_dataset = dataset_reader.read('./path/to/train/dataset')\n",
        "# Read in your validation dataset\n",
        "validation_dataset = dataset_reader.read('./path/to/validation/dataset')\n",
        "\n",
        "# Instantiate your custom model\n",
        "model = CustomModel()\n",
        "# Use the AllenNLP Trainer to train your model\n",
        "trainer = Trainer(model=model,\n",
        "                  optimizer=optimizer,\n",
        "                  train_dataset=train_dataset,\n",
        "                  validation_dataset=validation_dataset)\n",
        "trainer.train()\n",
        "\n",
        "# Once your model is trained, use it to make predictions on a test dataset\n",
        "predictor = Predictor.from_path(\"path/to/your/model/directory\", dataset_reader=dataset_reader)\n",
        "test_dataset = dataset_reader.read('./path/to/test/dataset')\n",
        "predictions = predictor.predict_batch_json(test_dataset)\n",
        "\n",
        "# Compare the predicted answers to the actual answers to calculate accuracy and precision\n",
        "correct_count = 0\n",
        "total_count = 0\n",
        "for example, prediction in zip(test_dataset, predictions):\n",
        "    if example['answer'] == prediction['answer']:\n",
        "        correct_count += 1\n",
        "    total_count += 1\n",
        "\n",
        "accuracy = correct_count / total_count\n",
        "print(\"Accuracy: \", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "WSf94ixV8WkP",
        "outputId": "1a4e64e5-391d-4a70-e88c-c77324d0226f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-a1784861ad69>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    class CustomModel(Model):\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "88O4vHdY8yQl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}